{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9df726a5",
   "metadata": {},
   "source": [
    "## Importing Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a14cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import random\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "import datetime\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f31d0fb",
   "metadata": {},
   "source": [
    "## Defining Data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a39338",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd() + '\\\\RTAI\\\\dataset\\\\'\n",
    "Classes = [\"Alpaca\", \"Not Alpaca\"]\n",
    "image_size = 28\n",
    "X = list()\n",
    "y = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8769df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_dir):\n",
    "    train_data = []\n",
    "    X = []\n",
    "    y = []\n",
    "    for each in Classes:\n",
    "        Class_id = Classes.index(each)\n",
    "        path = os.path.join(data_dir, each)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "            converted_array = cv2.resize(img_array, (image_size, image_size))\n",
    "            train_data.append([converted_array, Class_id])\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "    \n",
    "    for feature, label in train_data:\n",
    "        X.append(feature)\n",
    "        y.append(label)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa3620b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_dataset(data_dir)\n",
    "X_dup = np.array(X)\n",
    "assert(X.shape[0] == y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80a9ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X).reshape(-1 , 1, image_size, image_size)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5a77fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_val = tf.cast(X_train, tf.float32), tf.cast(X_test,tf.float32), tf.cast(X_val,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfd839bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(228, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "len(X_train)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15196ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "620bc42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e69d85",
   "metadata": {},
   "source": [
    "## Custom Layer and Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17868f34",
   "metadata": {},
   "source": [
    "### Custom Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6d1afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Proposed_Layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters):\n",
    "        super(Proposed_Layer, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.weight = self.add_weight(\n",
    "            shape=(self.filters, input_shape[-1], input_shape[-1]),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.bn(inputs)\n",
    "        samples, fmaps_stored = tf.shape(x)[0], tf.shape(x)[1]\n",
    "        resultant = tf.zeros((samples, self.filters, tf.shape(x)[2]-2, tf.shape(x)[3]-2))\n",
    "        resultant_shape = tf.shape(resultant)\n",
    "        dim_sample, dim_channel, dim_row, dim_col = resultant_shape[0], resultant_shape[1], resultant_shape[2], resultant_shape[3]\n",
    "\n",
    "        for n in range(dim_sample):\n",
    "            for k in range(dim_channel):\n",
    "                for i in range(dim_row):\n",
    "                    for j in range(dim_col):\n",
    "                        total_weight = tf.constant(0.0)\n",
    "                        for each_param in range(fmaps_stored):\n",
    "                            x1, y1, x2, y2 = i, i+3, j, j+3\n",
    "                            kernel = x[n, each_param, x1:y1, x2:y2]\n",
    "                            weights = self.weight[k, x1:y1, x2:y2]\n",
    "                            total_weight += tf.math.reduce_sum(tf.matmul(kernel, weights))\n",
    "                        resultant = tf.tensor_scatter_nd_add(resultant, [[n, k, i, j]], [total_weight])\n",
    "\n",
    "        return tf.reshape(resultant, (dim_sample, dim_channel, dim_row, dim_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0b0176",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0294e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Defining the Sequential model\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Adding a custom layer with 16 filters\n",
    "    model.add(Proposed_Layer(filters= 5))\n",
    "\n",
    "    # Adding a ReLU activation layer\n",
    "    model.add(tf.keras.layers.Activation(activations.relu))\n",
    "\n",
    "    # Adding a MaxPooling2D layer to downsample the feature maps\n",
    "    model.add(tf.keras.layers.MaxPooling2D(\n",
    "        pool_size=(2, 2), strides=(2, 2),\n",
    "    ))\n",
    "\n",
    "    # Adding another custom layer with 12 filters\n",
    "    model.add(Proposed_Layer(filters=3))\n",
    "\n",
    "    # Adding a ReLU activation layer\n",
    "    model.add(tf.keras.layers.Activation(activations.relu))\n",
    "\n",
    "    # Adding another MaxPooling2D layer to downsample the feature maps\n",
    "    model.add(tf.keras.layers.MaxPooling2D(\n",
    "        pool_size=(2, 2), strides=(2, 2),\n",
    "    ))\n",
    "\n",
    "    # Adding another custom layer with 8 filters and no activation function\n",
    "    # This layer generates 8 feature maps\n",
    "    model.add(Proposed_Layer(filters=2))\n",
    "\n",
    "    # Adding a ReLU activation layer\n",
    "    model.add(layers.Activation(activations.relu))\n",
    "\n",
    "    # Adding a Flatten layer to convert the 2D feature maps into a 1D array\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Adding a fully connected Dense layer with 16 neurons and a ReLU activation\n",
    "    model.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "    # Adding an output layer with one neuron and a softmax activation\n",
    "    # This will predict the class probability for binary classification\n",
    "    model.add(layers.Dense(units=1, activation=\"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ce1d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(model_custom, alpha, epoch, batch_size = None, adam=False):\n",
    "    # Set optimizer based on whether adam is used or not\n",
    "    if adam:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=alpha)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=alpha)\n",
    "\n",
    "    # Compile the model using the chosen optimizer\n",
    "    model_custom.compile(optimizer=optimizer, loss='BinaryCrossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Set up directory to store run files for TensorBoard\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # Train the model and save the training history\n",
    "    with tf.summary.create_file_writer(log_dir).as_default():\n",
    "        model_history = model_custom.fit(X_train, y_train,\n",
    "                                     epochs=epoch,\n",
    "                                     batch_size=batch_size,\n",
    "                                     validation_data=(X_val, y_val),\n",
    "                                     callbacks=[tensorboard_callback])\n",
    "\n",
    "    # Calculate and save the training accuracy\n",
    "    train_acc = np.mean(model_history.history['accuracy'])\n",
    "    \n",
    "    # Evaluate the model's performance on the test data\n",
    "    test_loss, test_acc = model_custom.evaluate(X_test, y_test)\n",
    "    train_loss = model_history.history['loss']\n",
    "\n",
    "    # Save results in a dictionary\n",
    "    results = {\"Learning Rate\": alpha, \"Batch Size\": batch_size, \"Epoch\": epoch,\n",
    "                  \"Train Accuracy\": train_acc, \"Test Accuracy\": test_acc, \"Train Loss\": train_loss, \"Test Loss\": test_acc}\n",
    "\n",
    "    return model_history, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2966fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636220e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_history1, results1 = model_builder(model_custom, alpha = 0.05, epoch = 5, batch_size = 32, adam=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5f45a6",
   "metadata": {},
   "source": [
    "# RUN ONLY UNTIL ABOVE CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7858cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "4/4 [==============================] - 415s 101s/step - loss: 0.7841 - accuracy: 0.5965 - val_loss: 79.3690 - val_accuracy: 0.4400\n",
      "Epoch 2/6\n",
      "4/4 [==============================] - 407s 100s/step - loss: 0.5967 - accuracy: 0.5965 - val_loss: 63.5287 - val_accuracy: 0.4400\n",
      "Epoch 3/6\n",
      "4/4 [==============================] - 414s 103s/step - loss: 0.5424 - accuracy: 0.5965 - val_loss: 32.8001 - val_accuracy: 0.4400\n",
      "Epoch 4/6\n",
      "4/4 [==============================] - 387s 96s/step - loss: 0.5333 - accuracy: 0.5965 - val_loss: 33.5009 - val_accuracy: 0.4400\n",
      "Epoch 5/6\n",
      "4/4 [==============================] - 385s 95s/step - loss: 0.5169 - accuracy: 0.5965 - val_loss: 37.8529 - val_accuracy: 0.4400\n",
      "Epoch 6/6\n",
      "4/4 [==============================] - 386s 95s/step - loss: 0.4761 - accuracy: 0.5965 - val_loss: 35.8348 - val_accuracy: 0.4400\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_history2, results2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_custom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [12], line 27\u001b[0m, in \u001b[0;36mmodel_builder\u001b[1;34m(model_custom, alpha, epoch, batch_size, adam)\u001b[0m\n\u001b[0;32m     24\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(model_history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Evaluate the model's performance on the test data\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_nn\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n\u001b[0;32m     28\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m model_history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Save results in a dictionary\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_nn' is not defined"
     ]
    }
   ],
   "source": [
    "model_history2, results2 = model_builder(model_custom, alpha = 0.05, epoch = 6, batch_size = 64, adam=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b80131",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history3, results3 = model_builder(model_custom, alpha = 0.1, epoch = 5, batch_size = 32, adam=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history4, results4 = model_builder(model_custom, alpha = 0.1, epoch = 6, batch_size = 64, adam=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2b122",
   "metadata": {},
   "source": [
    "## Reducing Model depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ab2545",
   "metadata": {},
   "source": [
    "#### Re-writing the create model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79add99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Defining the Sequential model\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Adding another custom layer with 12 filters\n",
    "    model.add(Proposed_Layer(filters=4))\n",
    "\n",
    "    # Adding a ReLU activation layer\n",
    "    model.add(tf.keras.layers.Activation(activations.relu))\n",
    "\n",
    "    # Adding another MaxPooling2D layer to downsample the feature maps\n",
    "    model.add(tf.keras.layers.MaxPooling2D(\n",
    "        pool_size=(2, 2), strides=(2, 2),\n",
    "    ))\n",
    "\n",
    "    # Adding another custom layer with 8 filters\n",
    "    # This layer generates 8 feature maps\n",
    "    model.add(Proposed_Layer(filters=2))\n",
    "\n",
    "    # Adding a ReLU activation layer\n",
    "    model.add(layers.Activation(activations.relu))\n",
    "\n",
    "    # Adding a Flatten layer to convert the 2D feature maps into a 1D array\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Adding a fully connected Dense layer with 16 neurons and a ReLU activation\n",
    "    model.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "    # Adding an output layer with one neuron and a softmax activation\n",
    "    # This will predict the class probability for binary classification\n",
    "    model.add(layers.Dense(units=1, activation=\"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c48d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_depth = create_model()\n",
    "model_historyd, resultsd = model_builder(model_depth, alpha = 0.1, epoch = 5, batch_size = 32, adam=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db6f166",
   "metadata": {},
   "source": [
    "## Decreasing Model Width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904c1930",
   "metadata": {},
   "source": [
    "#### Re-writing the create model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68330a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Defining the Sequential model\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Adding a custom layer with 8 filters\n",
    "    model.add(Proposed_Layer(filters= 4))\n",
    "\n",
    "    # Adding a ReLU activation layer\n",
    "    model.add(tf.keras.layers.Activation(activations.relu))\n",
    "\n",
    "    # Adding a MaxPooling2D layer to downsample the feature maps\n",
    "    model.add(tf.keras.layers.MaxPooling2D(\n",
    "        pool_size=(2, 2), strides=(2, 2),\n",
    "    ))\n",
    "\n",
    "    # Adding another custom layer with 4 filters\n",
    "    model.add(Proposed_Layer(filters=3))\n",
    "\n",
    "    # Adding a ReLU activation layer\n",
    "    model.add(tf.keras.layers.Activation(activations.relu))\n",
    "\n",
    "    # Adding another MaxPooling2D layer to downsample the feature maps\n",
    "    model.add(tf.keras.layers.MaxPooling2D(\n",
    "        pool_size=(2, 2), strides=(2, 2),\n",
    "    ))\n",
    "\n",
    "    # Adding another custom layer with 2 filters and no activation function\n",
    "    # This layer generates 2 feature maps\n",
    "    model.add(Proposed_Layer(filters=2))\n",
    "\n",
    "    # Adding a ReLU activation layer\n",
    "    model.add(layers.Activation(activations.relu))\n",
    "\n",
    "    # Adding a Flatten layer to convert the 2D feature maps into a 1D array\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Adding a fully connected Dense layer with 16 neurons and a ReLU activation\n",
    "    model.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "    # Adding an output layer with one neuron and a softmax activation\n",
    "    # This will predict the class probability for binary classification\n",
    "    model.add(layers.Dense(units=1, activation=\"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825952b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_width = create_model()\n",
    "model_historyw, resultsw = model_builder(model_width, alpha = 0.1, epoch = 5, batch_size = 32, adam=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1733b27",
   "metadata": {},
   "source": [
    "## Default CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134fb156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Defining the Sequential model\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Adding a Convolution layer with 16 filters\n",
    "    model.add(Conv2D(filters= 4, kerne_size=(3,3)))\n",
    "\n",
    "    # Adding a ReLU activation layer\n",
    "    model.add(tf.keras.layers.Activation(activations.relu))\n",
    "\n",
    "    # Adding a MaxPooling2D layer to downsample the feature maps\n",
    "    model.add(tf.keras.layers.MaxPooling2D(\n",
    "        pool_size=(2, 2), strides=(2, 2),\n",
    "    ))\n",
    "\n",
    "    # Adding a Convolution layer with 16 filters\n",
    "    model.add(Conv2D(filters= 3, kerne_size=(3,3)))\n",
    "\n",
    "    # Adding a ReLU activation layer\n",
    "    model.add(tf.keras.layers.Activation(activations.relu))\n",
    "\n",
    "    # Adding another MaxPooling2D layer to downsample the feature maps\n",
    "    model.add(tf.keras.layers.MaxPooling2D(\n",
    "        pool_size=(2, 2), strides=(2, 2),\n",
    "    ))\n",
    "\n",
    "    # Adding a Convolution layer with 16 filters\n",
    "    model.add(Conv2D(filters= 2, kerne_size=(3,3)))\n",
    "\n",
    "    # Adding a ReLU activation layer\n",
    "    model.add(layers.Activation(activations.relu))\n",
    "\n",
    "    # Adding a Flatten layer to convert the 2D feature maps into a 1D array\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Adding a fully connected Dense layer with 16 neurons and a ReLU activation\n",
    "    model.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "    # Adding an output layer with one neuron and a softmax activation\n",
    "    # This will predict the class probability for binary classification\n",
    "    model.add(layers.Dense(units=1, activation=\"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e244c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN = create_model()\n",
    "model_historyc, resultsc = model_builder(model_CNN, alpha = 0.1, epoch = 5, batch_size = 32, adam=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f460d5ac",
   "metadata": {},
   "source": [
    "## PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ffa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(train, val):\n",
    "# plot the accuracies\n",
    "    plt.plot(train_acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "\n",
    "# add labels and title\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training vs Validation Accuracy')\n",
    "\n",
    "# add legend\n",
    "    plt.legend()\n",
    "\n",
    "# show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237e6a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(results['Train Accuracy'], results['Val Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df5daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(results1['Train Accuracy'], results1['Val Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa6a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(results2['Train Accuracy'], results2['Val Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531774e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(results3['Train Accuracy'], results3['Val Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a251ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(results4['Train Accuracy'], results4['Val Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b65fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(resultsd['Train Accuracy'], resultsd['Val Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01142470",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(resultsw['Train Accuracy'], resultsw['Val Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49075f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(resultsc['Train Accuracy'], resultsc['Val Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8763778",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(results1['Train Accuracy'], results1['Val Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1c2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(results2['Train Accuracy'], results2['Val Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fee95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(results3['Train Accuracy'], results3['Val Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ea0797",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(results4['Train Accuracy'], results4['Val Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcb7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(resultsw['Train Loss'], resultsw['Val Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87184ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(resultsd['Train Loss'], resultsd['Val Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665cfbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(resultsc['Train Loss'], resultsc['Val Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1969cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(results['Train Loss'], results['Val Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c7328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard dev upload --logdir ./logs \\\n",
    "--name \"RTAI_ASSIGNMENT_2\\All Trainings\" \\\n",
    "--description \"Logs data of all Trainings models on, \\\n",
    "different hyperparameters for proposed layer, \\\n",
    "decrease in width, decrease in depth and default cnn\" \\\n",
    "--one_shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a9ce7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f47c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b90cac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.13'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da080b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard dev upload --logdir ./logs \\\n",
    "  --name \"Simple experiment with MNIST\" \\\n",
    "  --description \"Training results from https://colab.sandbox.google.com/github/tensorflow/tensorboard/blob/master/docs/tbdev_getting_started.ipynb\" \\\n",
    "  --one_shot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
